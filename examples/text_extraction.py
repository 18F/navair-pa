"""
Requirements:
A virtualenv is highly recommended.

If you don't have the required imports installed it will error. 
`pip install` as needed. See text_analysis.py for more details.

Possibly useful: https://spacy.io/usage/
https://valiancesolutions.com/whitepapers/automated-tagging-of-articles-using-unsupervised-learning/
https://pypi.org/project/topia.termextract/

Note: this is for an example only. To meaningfully extract phrases you'd need a more complex set of chunker 
rules, and you'd have to better train it. See the NLTK docs for more details. 

"""
import operator
import PyPDF2


import nltk
from nltk import FreqDist
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.stem.porter import PorterStemmer
from nltk.tokenize import word_tokenize

import matplotlib
# In some cases you may run into some weirdness in installed 
# python versions, matplotlib and a "not installed as a framework" error.
# If so, the command below resolves the backend.
# Note that it MUST be run before any further imports.
matplotlib.use('TkAgg')
# You may now continue importing
import matplotlib.pyplot as plt

# make sure NLTK modules are loaded
# If you've done this recently, these can be safely commented out.
# nltk.download('punkt')
# nltk.download('stopwords')
# nltk.download('wordnet')
# nltk.download('averaged_perceptron_tagger')

grammar = r"""
        NBAR:
            {<NN.*|JJ>*<NN.*>}  # Nouns and Adjectives, terminated with Nouns
            
        NP:
            {<NBAR>}
            {<NBAR><IN><NBAR>}  # Above, connected with in/of/etc...
    """
chunker = nltk.RegexpParser(grammar)


def extract_text(pd_file=None):
    """
    Given a PDF or text file, extract the text from it.
    Be sure to set the `pd_file` and `pd_directory` variables as needed.
    """
    if not pd_file:
        pd_file = '2892-10 AIRCRAFT ELECTRICIAN.pdf'
    pd_directory = 'pd_files/'
    pd_file_path = pd_directory + pd_file
    
    # determine the file type and read accordingly.
    try:
        suffix = pd_file_path.rsplit('.', 1)[1].lower()
    except IndexError:
        print("No suffix: We don't know what the file type is.")
        return
    if suffix == 'pdf':
        # Open and read the file into pyPDF2
        pd_file = open(pd_file_path,'rb')
        pd = PyPDF2.PdfFileReader(pd_file)

        #Iterate through PDF pages and extract
        pd_text = ''
        for page in pd.pages:
            pd_text += page.extractText()
    elif suffix == "txt":
        pd_file = open(pd_file_path,'r')
        pd_text = pd_file.read()
    else:
        print("Unknown file type. Please use PDF or TXT files")
        return
    return clean_text(pd_text)


def clean_text(text):
    """
    Given a block of text, clean it to remove unwanted characters other cruft.
    This will:
    1. Strip newlines - we don't care about them for word counts, 
    2. Cast to lowercase for consistency.
    3. Replace / with a space, retaining the words on either side of the /
    4. Replace "smart" quotes and apostrophes with dumb ones.
    """
    text = text.replace('\n', '').lower()
    text = text.replace('/', " ")
    text = text.replace('’', "'")
    return text


def clean_tokens(tokens):
    """
    Given a list of word tokens generated by `nltk.word_tokenize`, 
    cleans out unwanted punctuation and stopwords.
    """
    punctuation = ['(', ')', ';', ':', '[', ']', ',', '.', '#', '%', "'s", "’", '!', '``']
    stop_words = stopwords.words('english')
    tokens = [word for word in tokens if not word in stop_words and not word in punctuation]
    stemmer = PorterStemmer()
    stemmed_words = [stemmer.stem(word) for word in tokens]
    lemmatizer = WordNetLemmatizer()
    lem_words = [lemmatizer.lemmatize(word) for word in stemmed_words]
    # and now a second pass to pick up any verbs remaining
    keywords = [lemmatizer.lemmatize(word, pos='v') for word in lem_words]
    return keywords



def tag_sentences(pd_text):
    """
    Returns tokenized words, broken down by sentence, 
    and tagged by part of speech.
    See https://www.nltk.org/book/ch07.html
    """
    tagged_sentences = []
    sentences = nltk.sent_tokenize(pd_text)
    # We need to tokenize, clean and tag parts of speech for the words in each sentence. 
    # For clarity, we'll do it in a loop instead of a list comprehension
    # so it's easier to see each step.
    for sentence in sentences:
        token_list = nltk.word_tokenize(sentence)
        cleaned = clean_tokens(token_list)
        tagged = nltk.pos_tag(cleaned)
        tagged_sentences.append(tagged)
    return tagged_sentences


def tag_words(pd_text):
    """
    Returns simple list of tokenized words, cleaned and tagged by part of speech.
    See https://www.nltk.org/book/ch07.html
    """
    tokens = nltk.word_tokenize(pd_text)
    tokens = clean_tokens(tokens)
    return nltk.tag.pos_tag(tokens)


def extract_phrases(pd_text=None):
    if not pd_text:
        pd_text = extract_text()
    tagged_sentences = tag_sentences(pd_text)
    leaves = []
    """Find NP (nounphrase) leaf nodes of the chunk tree."""
    for sentence in tagged_sentences:
        tree = chunker.parse(sentence)
        for subtree in tree.subtrees(filter=lambda t: t.label() == 'NP'):
            leafwords = [ w for w,t in subtree.leaves() ]
            leaves.append(' '.join(leafwords))
    return leaves


def get_keyword_frequency():
    """
    Given a block of text from a document, get the keywords.
    """
    # extract and clean the keyword tokens with NLTK
    keywords = extract_phrases()
    freq = nltk.FreqDist(keywords)

    for k, v in freq.most_common(50):
        print(f'{k:<20} {v}')


if __name__ == "__main__":
    # execute only if run as a script
    get_keyword_frequency()